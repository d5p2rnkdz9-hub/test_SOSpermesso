---
phase: 02-adaptive-logic-results
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/lib/rules-engine.ts
  - src/app/api/feedback/route.ts
  - src/app/quiz/results/page.tsx
  - src/components/quiz/FeedbackDisplay.tsx
  - src/components/quiz/CoursePrompts.tsx
autonomous: false

must_haves:
  truths:
    - "User receives personalized feedback based on their specific answer patterns (not generic)"
    - "User receives 1-2 actionable course prompts when gaps are detected (not homework)"
    - "Feedback is professional assessment tone, 2-3 paragraphs, no scores or levels"
    - "Facilitator notes are generated and stored for instructor access"
    - "Results page displays course prompts separately from main feedback"
  artifacts:
    - path: "src/lib/rules-engine.ts"
      provides: "Rules engine that evaluates responses and detects gaps"
      exports: ["evaluateResponses", "RulesResult"]
    - path: "src/app/api/feedback/route.ts"
      provides: "Enhanced feedback API using rules engine output"
      contains: "evaluateResponses"
    - path: "src/components/quiz/CoursePrompts.tsx"
      provides: "UI component for displaying course prompts"
      contains: "CoursePrompt"
    - path: "src/app/quiz/results/page.tsx"
      provides: "Results page showing feedback + course prompts"
      contains: "CoursePrompts"
  key_links:
    - from: "src/lib/rules-engine.ts"
      to: "src/app/api/feedback/route.ts"
      via: "evaluateResponses called before Claude prompt"
      pattern: "evaluateResponses"
    - from: "src/app/api/feedback/route.ts"
      to: "prisma.session.update"
      via: "Stores coursePrompts and facilitatorNotes in session"
      pattern: "coursePrompts.*facilitatorNotes"
    - from: "src/app/quiz/results/page.tsx"
      to: "src/components/quiz/CoursePrompts.tsx"
      via: "Renders course prompts from API response"
      pattern: "CoursePrompts"
---

<objective>
Build a rules engine to evaluate quiz responses and enhance the feedback system to deliver personalized, professional feedback with optional course prompts.

Purpose: Move beyond generic Claude feedback to a system where answer patterns are analyzed by a rules engine, gap areas are detected, and Claude generates targeted professional feedback using that analysis. Course prompts give participants actionable notes for during the course, and facilitator notes help instructors prepare.

Output:
- Rules engine module that evaluates responses and identifies gaps/strengths
- Rewritten Claude prompt for professional assessment tone (2-3 paragraphs, no levels)
- Course prompts generation and storage
- Facilitator notes generation and storage
- Updated results page showing feedback + course prompts
</objective>

<execution_context>
@/Users/albertopasquero/.claude/get-shit-done/workflows/execute-plan.md
@/Users/albertopasquero/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-adaptive-logic-results/02-CONTEXT.md
@.planning/phases/02-adaptive-logic-results/02-01-SUMMARY.md
@src/app/api/feedback/route.ts
@src/app/quiz/results/page.tsx
@src/components/quiz/FeedbackDisplay.tsx
@src/types/quiz.ts
@prisma/schema.prisma
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rules engine + enhanced feedback API</name>
  <files>src/lib/rules-engine.ts, src/app/api/feedback/route.ts</files>
  <action>
  **Create rules engine (src/lib/rules-engine.ts):**

  This module evaluates a participant's responses and produces a structured analysis that feeds the Claude prompt. It does NOT assign scores or levels (per CONTEXT.md decisions).

  ```typescript
  export interface RulesResult {
    // Profile summary
    awarenessLevel: "none" | "aware" | "user" | "active-user"
    pathTaken: "not-aware" | "aware-not-working" | "aware-working"

    // Detected characteristics
    toolsUsed: string[]           // Which AI tools they've used
    workActivities: string[]      // How they use AI for work
    usageTrend: string | null     // increasing/stable/decreasing/stopped
    satisfaction: string | null   // frustrated/neutral/satisfied/very-satisfied
    barriers: string[]            // What's stopping them
    concerns: string[]            // Privacy, ethics, reliability concerns
    priorities: string[]          // What they want to focus on in course
    expectations: string | null   // Free text expectations
    confidence: string | null     // beginner/intermediate/advanced

    // Gap detection
    gaps: GapArea[]               // Detected areas where course can help
  }

  export interface GapArea {
    area: string                  // e.g., "practical-usage", "risk-awareness", "prompt-skills"
    description: string           // Human-readable description (Italian) for Claude context
    severity: "minor" | "significant"  // How much attention this area needs
  }
  ```

  **Gap detection rules:**
  The rules engine analyzes answer combinations to detect gaps:

  1. **No awareness** (Q1 = No): Gap "foundational-knowledge" (significant) - needs basics before anything else
  2. **Aware but never used for work** (Q2 = No): Gap "practical-application" (significant) - knows tools exist but hasn't applied them
  3. **Barriers include privacy/reliability fears** (Q2d contains privacy or reliability): Gap "risk-understanding" (minor) - needs reassurance through knowledge
  4. **No tools used** or **only one tool**: Gap "tool-breadth" (minor) - would benefit from seeing multiple tools
  5. **Confidence = beginner**: Gap "hands-on-practice" (significant) - needs guided practice
  6. **Concerns include deontological**: Gap "ethical-framework" (minor) - course covers this, flag for emphasis
  7. **Satisfaction = frustrated or poco**: Gap "effective-usage" (significant) - using AI but not getting value
  8. **Usage trend = decreasing or stopped**: Gap "re-engagement" (significant) - tried but gave up

  The function signature:
  ```typescript
  export function evaluateResponses(
    answers: Array<{ questionId: string; value: unknown; questionText: string; questionOptions: unknown }>,
  ): RulesResult
  ```

  Parse the answers by questionId to extract structured data, then apply gap detection rules.

  **Enhance feedback API (src/app/api/feedback/route.ts):**

  1. After fetching session + answers, call `evaluateResponses()` with the answer data
  2. Build an enhanced Claude prompt using the RulesResult:

  The new Claude prompt should:
  - Receive the RulesResult summary (not raw answers)
  - Generate feedback in Italian with these sections:
    a) **Main feedback** (2-3 paragraphs): Professional assessment tone. Reference their path (what they've tried, what their experience level suggests). Neutral, factual, encouraging. No scores, levels, or judgments. About 200 words.
    b) **Course prompts** (0-2 prompts): ONLY if gaps detected. Phrased as "Durante il corso, chiedi ai docenti di..." or "Presta attenzione quando si parlera di...". Return as JSON array within the response.
    c) **Facilitator notes** (1 paragraph): Brief summary for the instructor about what this participant needs, in Italian. Not shown to participant.

  Request Claude to return a JSON object:
  ```json
  {
    "feedback": "...(main feedback text)...",
    "coursePrompts": [{"text": "...", "category": "practical-usage"}],
    "facilitatorNotes": "..."
  }
  ```

  Use Claude model `claude-3-haiku-20240307` (same as Phase 1 - cost-effective).
  Set max_tokens to 1500 (increased from 1024 for structured response).

  3. Parse the Claude JSON response
  4. Store all three fields in the Session record:
     - `feedback` (existing field) <- main feedback text
     - `coursePrompts` <- JSON array of prompts
     - `facilitatorNotes` <- facilitator text

  5. Return to client: `{ feedback, coursePrompts, facilitatorNotes: null }` (do NOT expose facilitatorNotes to participant)

  6. Keep the graceful fallback behavior for missing API key or errors (same pattern as Phase 1)

  **Update the existing `buildAnswersContext` function**: Keep it but enhance to also pass question metadata. Better yet, replace its usage with the RulesResult summary since that's more structured for Claude.

  **Important anti-patterns to avoid:**
  - Do NOT use `response_format: { type: "json_object" }` - Haiku does not support this well. Instead, include clear instructions in the prompt to return JSON and parse with try/catch + fallback.
  - Do NOT expose facilitatorNotes in the client response. This is for admin dashboard (Phase 3).
  </action>
  <verify>
  - `npm run build` passes without TypeScript errors
  - Test with `curl -X POST http://localhost:3000/api/feedback -H "Content-Type: application/json" -d '{"sessionId":"<test-session-id>"}'` returns structured response with feedback, coursePrompts array
  - Verify coursePrompts are stored in session record (check via Prisma Studio: `npx prisma studio`)
  - Verify facilitatorNotes is NOT in the API response to client
  </verify>
  <done>
  - Rules engine evaluates responses and detects 0-N gaps
  - Claude receives structured analysis (not raw answers) for focused feedback generation
  - Feedback is 2-3 paragraphs, professional tone, no scores/levels
  - Course prompts generated only when relevant gaps detected
  - Facilitator notes stored in database but not exposed to participant
  - API returns { feedback, coursePrompts } to client
  </done>
</task>

<task type="auto">
  <name>Task 2: Results page with course prompts UI</name>
  <files>src/components/quiz/CoursePrompts.tsx, src/app/quiz/results/page.tsx, src/components/quiz/FeedbackDisplay.tsx</files>
  <action>
  **Create CoursePrompts component (src/components/quiz/CoursePrompts.tsx):**

  A component that displays 0-2 actionable course prompts. Only rendered when prompts exist.

  Design:
  - Card with a distinct visual style (different from feedback card - use a warm accent, e.g., amber/orange tones)
  - Header: icon (Lightbulb from lucide-react) + "Suggerimenti per il corso"
  - Each prompt displayed as a callout with an arrow or bullet
  - Phrasing is action-oriented: "Durante il corso..." / "Presta attenzione..."
  - Clean, not overwhelming - these are secondary to the main feedback
  - Props: `prompts: Array<{ text: string; category: string }>`
  - If prompts array is empty or null, render nothing (return null)

  **Update results page (src/app/quiz/results/page.tsx):**

  1. Update the feedback fetch to handle the new response shape: `{ feedback, coursePrompts }`
  2. Store `coursePrompts` in component state alongside `feedback`
  3. Render `CoursePrompts` component below the `FeedbackDisplay` (only if prompts exist)
  4. Layout order: Success header -> FeedbackDisplay -> CoursePrompts -> Restart button
  5. Keep all existing functionality (redirect logic, loading states, error handling, restart)

  **Minor update to FeedbackDisplay (src/components/quiz/FeedbackDisplay.tsx):**
  No structural changes needed. The feedback text format hasn't changed - it's still a string rendered as paragraphs. Keep existing component as-is unless the AI output format requires markdown parsing adjustments.

  **Export CoursePrompts from quiz index (src/components/quiz/index.ts):**
  Add export for the new component.
  </action>
  <verify>
  - `npm run build` passes without errors
  - Results page renders main feedback
  - Results page renders course prompts below feedback (when present)
  - Results page shows NO course prompts section when array is empty
  - Mobile responsive - check on narrow viewport
  </verify>
  <done>
  - CoursePrompts component renders 0-2 actionable prompts with distinct styling
  - Results page fetches and displays both feedback and coursePrompts
  - Empty prompts array results in no extra UI (clean experience)
  - All existing results page functionality preserved (loading, error, restart)
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete adaptive quiz flow with branching, rules-based feedback, and course prompts</what-built>
  <how-to-verify>
  1. Open http://localhost:3000 and start a new quiz
  2. **Test Path A (not aware):** Answer Q1 "No" -> should see Q1b -> then jump to Q5 -> Q6 -> complete. Should see ~4 questions total.
  3. **Test Path C (full path):** Restart quiz. Answer Q1 "Yes" -> Q1a (select tools) -> Q2 "Yes" -> Q2a, Q2b, Q2c -> Q3 -> Q4 -> Q5 -> Q6 -> complete. Should see ~10 questions.
  4. **Check feedback:** After completing, verify:
     - Feedback is 2-3 paragraphs, professional tone, in Italian
     - NO scores, levels, or categorization
     - Course prompts appear below feedback (if relevant)
     - Prompts are actionable ("Durante il corso...")
  5. **Check progress bar:** During quiz, verify the total count matches actual path length
  6. **Check Italian text:** All questions and feedback in correct Italian
  </how-to-verify>
  <resume-signal>Type "approved" to complete phase, or describe any issues found</resume-signal>
</task>

</tasks>

<verification>
1. `npm run build` - no TypeScript errors
2. Rules engine correctly categorizes users across all three paths
3. Claude generates structured JSON response with feedback + coursePrompts + facilitatorNotes
4. Results page displays feedback and optional course prompts
5. Facilitator notes stored in DB but NOT shown to participant
6. Full quiz flow works end-to-end: start -> branch -> complete -> feedback + prompts
</verification>

<success_criteria>
- Rules engine evaluates responses and identifies gaps from answer patterns
- Feedback is personalized, professional, 2-3 paragraphs, no scores/levels
- Course prompts appear only when gaps detected (1-2 max)
- Facilitator notes stored for Phase 3 admin dashboard
- Results page renders both feedback and course prompts
- All three quiz paths produce appropriate, distinct feedback
</success_criteria>

<output>
After completion, create `.planning/phases/02-adaptive-logic-results/02-02-SUMMARY.md`
</output>
